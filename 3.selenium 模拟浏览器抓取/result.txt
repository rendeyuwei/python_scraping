
第 1 页评论:
乌拉乌拉 球形猪！
嘻嘻，留个爪爪！
我就是想看看会不会留下IP
好爱王老师！
彭乐威牛逼
Unable to locate element: iframe[title='livere']
为什么我用的是源码能请求打开浏览器但是他不能定位到这个iframe？
各路大神支支招！
@人在逋 因为网页更新了 试试iframe[title='livere-comment']
@小雨过天阴 不管用啊
"isMobile": "0",
"isSecret": 0,
"isModified": 0,
"confirm": 0,
"subCount": 0,


1072479416 2020.11.19 到此一游
求助~用selenium模拟登录chrome显示“chrome正受到自动测试软件的控制”，浏览器版本V86
鹤鹤子到此一游 呵呵 ^_^
@嫌我烦了是呗 小傻子
hhhhhh 踩踩

第 2 页评论:
太牛了啊你们
我刘佳全球最美
我钟丽诗宇宙无敌
魏灏最厉害
佳佳小怪兽可可爱
2020.11.10 测试一下是否有用~~~
网页结构都变了，这上面第四章代码不太管用了，感觉这个写的还蛮不错的，也是依据这本书的案例写的，实测可以运行，大家有兴趣可以看看。https://blog.csdn.net/weixin_43616817/article/details/109022479
前来测试
111
livere已经改了，后来的小白同学可以在检查页面看一下

第 3 页评论:
from selenium import webdriver
import time

#获取webdriver对象，使用火狐浏览器
firefox = webdriver.Firefox()
firefox.implicitly_wait(30)
firefox.get("http://www.santostang.com/2018/07/04/hello-world/")
print('连接成功')

#封装程序
def get_comments(n,m):

if n

11
offset现在找不到了，请问老师当这种下一页找不到规律的时候该怎么继续？
@albanwang 只有第一页的offset看不到，其它页的都有的，也就是规律没变。
测试中,关键点:
comment_list=json_data['results']['parents']
原来要梯子才能加载出评论zz
测试路过
有没有大佬看下，用书上P58-59代码怎么只爬取了第六页的评论

爬虫...爬...
hyui
y,路^

第 4 页评论:
from selenium import webdriver
import time

#获取webdriver对象，使用火狐浏览器
firefox = webdriver.Firefox()
firefox.implicitly_wait(30)
firefox.get("http://www.santostang.com/2018/07/04/hello-world/")
print('连接成功')

#封装程序
def get_comments(n,m):

if n

11
offset现在找不到了，请问老师当这种下一页找不到规律的时候该怎么继续？
@albanwang 只有第一页的offset看不到，其它页的都有的，也就是规律没变。
测试中,关键点:
comment_list=json_data['results']['parents']
原来要梯子才能加载出评论zz
测试路过
有没有大佬看下，用书上P58-59代码怎么只爬取了第六页的评论

爬虫...爬...
hyui
y,路^

第 5 页评论:
from selenium import webdriver
import time

#获取webdriver对象，使用火狐浏览器
firefox = webdriver.Firefox()
firefox.implicitly_wait(30)
firefox.get("http://www.santostang.com/2018/07/04/hello-world/")
print('连接成功')

#封装程序
def get_comments(n,m):

if n

11
offset现在找不到了，请问老师当这种下一页找不到规律的时候该怎么继续？
@albanwang 只有第一页的offset看不到，其它页的都有的，也就是规律没变。
测试中,关键点:
comment_list=json_data['results']['parents']
原来要梯子才能加载出评论zz
测试路过
有没有大佬看下，用书上P58-59代码怎么只爬取了第六页的评论

爬虫...爬...
hyui
y,路^

第 6 页评论:
我发现一个和有意思的事，各位大佬发在评论里的代码是没有缩进的，不过爬取到的评论内容格式正确！！！
爬虫原来还有这种妙用！太棒了哈哈哈哈哈
动态抓取来的(　o=^ェ)o　┏━┓
@穿山甲 朋友，成功了吗
@REVIN 成功了！哈哈哈
122
你好帅哥
555
hello
hello
开心开心
测试
抛砖引玉一下：可以获取自定义起始页和终止页的评论
#!/usr/bin/env python
# -*- conding:utf-8 -*-
# button.click() 之后也要等上几秒再切换frame，不然有时候会加载不出来
# time.sleep(2)
# browser.switch_to.parent_frame()


from selenium import webdriver
import time

browser = webdriver.Firefox() # geckodriver.exe设置在环境路径中
browser.implicitly_wait(20) # 隐性等待，最长等20秒
url = ''
browser.get(url)


def get_to_startpage(i):
n_next = (i - 1) // 10
while n_next > 0:
page = 'next'
browser.execute_script("window.scrollTo(0, document.body.scrollHeight)")
browser.switch_to.frame(browser.find_element_by_css_selector("iframe[title='livere']"))
button = browser.find_element_by_css_selector("button[data-page='{}']".format(page))
button.click()
time.sleep(2)
browser.switch_to.parent_frame()
n_next -= 1
print('next')
# browser.implicitly_wait(20)
time.sleep(2)
page = i
browser.execute_script("window.scrollTo(0, document.body.scrollHeight)")
browser.switch_to.frame(browser.find_element_by_css_selector("iframe[title='livere']"))
button = browser.find_element_by_css_selector("button[data-page='{}']".format(page))
button.click()
time.sleep(2)
browser.switch_to.parent_frame()
time.sleep(2)
print('找到起始页啦！')


def get_comments(i):
browser.execute_script("window.scrollTo(0, document.body.scrollHeight)")
browser.switch_to.frame(browser.find_element_by_css_selector("iframe[title='livere']"))
comments = browser.find_elements_by_css_selector('div.reply-content')
for each_comment in comments:
content = each_comment.find_element_by_tag_name('p').text
print(content)
if i % 10 == 0:
page = 'next'
else:
page = i + 1
button = browser.find_element_by_css_selector("button[data-page='{}']".format(page))
button.click()
time.sleep(2)
browser.switch_to.parent_frame()
time.sleep(2)


START_PAGE = 10
END_PAGE = 13


get_to_startpage(START_PAGE)
for page_n in range(START_PAGE, END_PAGE+1):
print("第%d页评论：" % page_n)
get_comments(page_n)
print('-'*50)
time.sleep(2)
print("第%d至第%d的评论打印成功啦！" % (START_PAGE, END_PAGE))

第 7 页评论:
我发现一个和有意思的事，各位大佬发在评论里的代码是没有缩进的，不过爬取到的评论内容格式正确！！！
爬虫原来还有这种妙用！太棒了哈哈哈哈哈
动态抓取来的(　o=^ェ)o　┏━┓
@穿山甲 朋友，成功了吗
@REVIN 成功了！哈哈哈
122
你好帅哥
555
hello
hello
开心开心
测试
抛砖引玉一下：可以获取自定义起始页和终止页的评论
#!/usr/bin/env python
# -*- conding:utf-8 -*-
# button.click() 之后也要等上几秒再切换frame，不然有时候会加载不出来
# time.sleep(2)
# browser.switch_to.parent_frame()


from selenium import webdriver
import time

browser = webdriver.Firefox() # geckodriver.exe设置在环境路径中
browser.implicitly_wait(20) # 隐性等待，最长等20秒
url = ''
browser.get(url)


def get_to_startpage(i):
n_next = (i - 1) // 10
while n_next > 0:
page = 'next'
browser.execute_script("window.scrollTo(0, document.body.scrollHeight)")
browser.switch_to.frame(browser.find_element_by_css_selector("iframe[title='livere']"))
button = browser.find_element_by_css_selector("button[data-page='{}']".format(page))
button.click()
time.sleep(2)
browser.switch_to.parent_frame()
n_next -= 1
print('next')
# browser.implicitly_wait(20)
time.sleep(2)
page = i
browser.execute_script("window.scrollTo(0, document.body.scrollHeight)")
browser.switch_to.frame(browser.find_element_by_css_selector("iframe[title='livere']"))
button = browser.find_element_by_css_selector("button[data-page='{}']".format(page))
button.click()
time.sleep(2)
browser.switch_to.parent_frame()
time.sleep(2)
print('找到起始页啦！')


def get_comments(i):
browser.execute_script("window.scrollTo(0, document.body.scrollHeight)")
browser.switch_to.frame(browser.find_element_by_css_selector("iframe[title='livere']"))
comments = browser.find_elements_by_css_selector('div.reply-content')
for each_comment in comments:
content = each_comment.find_element_by_tag_name('p').text
print(content)
if i % 10 == 0:
page = 'next'
else:
page = i + 1
button = browser.find_element_by_css_selector("button[data-page='{}']".format(page))
button.click()
time.sleep(2)
browser.switch_to.parent_frame()
time.sleep(2)


START_PAGE = 10
END_PAGE = 13


get_to_startpage(START_PAGE)
for page_n in range(START_PAGE, END_PAGE+1):
print("第%d页评论：" % page_n)
get_comments(page_n)
print('-'*50)
time.sleep(2)
print("第%d至第%d的评论打印成功啦！" % (START_PAGE, END_PAGE))

第 8 页评论:
+1
书中的应该是 iframe 而不是 ifram e
一脸懵逼的看着大佬装逼
第五条评论
第四条评论
第三条评论
第二条评论
第一条评论
新鲜出炉的代码，亲测可用！因为评论的排版问题，同时附上截图供大家参考。


from selenium import webdriver
import time

browser=webdriver.Chrome()

print("wait for link...")
browser.get("http://www.santostang.com/2018/07/04/hello-world/")
print("link ok!")
print("\nwait 5s...")
time.sleep(5)
browser.maximize_window() #最大化窗口
print("wait over")


for page in range(9,14):
print("\n页数：",end="")
print(page)
print("wait 30s...")
time.sleep(30)
print("wait over")

print("\n下滑到页面底部")
browser.execute_script("window.scrollTo(0, document.body.scrollHeight);") # 下滑到页面底部 需要-700
print("下滑成功")
print("\n转换iframe")
browser.switch_to.frame(browser.find_element_by_css_selector("iframe[title='livere']")) # 转换iframe
print("转换成功")
print("\n定位翻页按钮")
local = 'button[data-page="' + str(page) + '"]'
if page == 11:
local = 'button[data-page="next"]'
print(local)
load_more = browser.find_element_by_css_selector(local) # 定位翻页按钮
print("定位成功")
print("\n点击")
load_more.click() # 点击
print("点击成功")
print("wait 10s...")
time.sleep(10) # 等待1s加载
print("wait over")

print("\n寻找评论内容")
comments = browser.find_elements_by_css_selector('div.reply-content')
print("寻找成功")
print("\n输出信息")
for each_comment in comments:
content = each_comment.find_element_by_tag_name('p')
print(content.text)
print("切换到新页面")
browser.switch_to.default_content()
print("切换成功")
编写了一个自动爬取十页的程序，希望各位有所指教
import time
from selenium import webdriver
driver=webdriver.Chrome()
driver.get("http://www.santostang.com/2018/07/04/hello-world/")
time.sleep(5)
for page in range(0,10):
# 下滑到页面底部
driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
driver.switch_to_frame(driver.find_element_by_css_selector("iframe[title='livere']"))
for comment in driver.find_elements_by_css_selector("div.reply-content"):
content=comment.find_element_by_tag_name('p')
print(content.text)
if(page div.more-wrapper > ("
str2=str(page+2)
str3=")"
sfsfd=str1+str2+str3
driver.find_element_by_css_selector(sfsfd).click()
time.sleep(5)
driver.switch_to_default_content()

第 9 页评论:
+1
书中的应该是 iframe 而不是 ifram e
一脸懵逼的看着大佬装逼
第五条评论
第四条评论
第三条评论
第二条评论
第一条评论
新鲜出炉的代码，亲测可用！因为评论的排版问题，同时附上截图供大家参考。


from selenium import webdriver
import time

browser=webdriver.Chrome()

print("wait for link...")
browser.get("http://www.santostang.com/2018/07/04/hello-world/")
print("link ok!")
print("\nwait 5s...")
time.sleep(5)
browser.maximize_window() #最大化窗口
print("wait over")


for page in range(9,14):
print("\n页数：",end="")
print(page)
print("wait 30s...")
time.sleep(30)
print("wait over")

print("\n下滑到页面底部")
browser.execute_script("window.scrollTo(0, document.body.scrollHeight);") # 下滑到页面底部 需要-700
print("下滑成功")
print("\n转换iframe")
browser.switch_to.frame(browser.find_element_by_css_selector("iframe[title='livere']")) # 转换iframe
print("转换成功")
print("\n定位翻页按钮")
local = 'button[data-page="' + str(page) + '"]'
if page == 11:
local = 'button[data-page="next"]'
print(local)
load_more = browser.find_element_by_css_selector(local) # 定位翻页按钮
print("定位成功")
print("\n点击")
load_more.click() # 点击
print("点击成功")
print("wait 10s...")
time.sleep(10) # 等待1s加载
print("wait over")

print("\n寻找评论内容")
comments = browser.find_elements_by_css_selector('div.reply-content')
print("寻找成功")
print("\n输出信息")
for each_comment in comments:
content = each_comment.find_element_by_tag_name('p')
print(content.text)
print("切换到新页面")
browser.switch_to.default_content()
print("切换成功")
编写了一个自动爬取十页的程序，希望各位有所指教
import time
from selenium import webdriver
driver=webdriver.Chrome()
driver.get("http://www.santostang.com/2018/07/04/hello-world/")
time.sleep(5)
for page in range(0,10):
# 下滑到页面底部
driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
driver.switch_to_frame(driver.find_element_by_css_selector("iframe[title='livere']"))
for comment in driver.find_elements_by_css_selector("div.reply-content"):
content=comment.find_element_by_tag_name('p')
print(content.text)
if(page div.more-wrapper > ("
str2=str(page+2)
str3=")"
sfsfd=str1+str2+str3
driver.find_element_by_css_selector(sfsfd).click()
time.sleep(5)
driver.switch_to_default_content()

第 10 页评论:
si
test
+1
试下
谢谢作者如此负责任
学习动态抓取
234
123
测试评论 啦啦啦
小白请教一下各位大佬，比如说我现在有一批CSV文件（1000组），全部是出发地和目的地，如果我想实现用百度地图自动搜索所有的出发地和目的地并抓取所需时间的话，

关于如何批量把csv导入百度地图这个步骤，应该往哪个方向自学呢？提前感谢各位大大O.O

第 11 页评论:
selenium获取文章的所有评论这段代码运行错误：
load_more = driver.find_element_by_css_selector('button.more-btn')
提示：NoSuchElementException: Message: Unable to locate element: button.more-btn
这该如何解决
@林昕杰 改成：
load_more = driver.find_element_by_css_selector('button.page-btn ')
他的按钮标签名换掉了
just for test
# 20191203可以运行
from selenium import webdriver
import time #为了使用延时函数

url = "http://www.santostang.com/2018/07/04/hello-world/"
driver = webdriver.Firefox(executable_path = r'J:\Python编程\软件\geckodriver.exe') #根据自己电脑软件路径定义修改

print("wait for link...")
driver.get(url)
print("link ok!")
print("wait 5s...")
time.sleep(5)
driver.maximize_window() #最大化窗口
print("wait over")

for page in range(1, 19):
print("页数：")
print (page)
time.sleep(1)
driver.execute_script("window.scrollTo(0, document.body.scrollHeight - 700);") # 下滑到页面底部 需要-700

driver.switch_to.frame(driver.find_element_by_css_selector("iframe[title='livere']")) # 转换iframe
comments = driver.find_elements_by_css_selector('div.reply-content') # 找到评论内容
for eachcomment in comments:
content = eachcomment.find_element_by_tag_name('p')
print(content.text)



xpath = "//button[@data-page='" + str(page+1) + "']" # 定位翻页按钮

if page == 10: #第10页按钮变化
xpath = "//button[@data-page='"+ "next"+ "']" # 定位翻页按钮
load_more = driver.find_element_by_xpath(xpath)
load_more.click() # 点击
time.sleep(1) # 等待1s加载

driver.switch_to.default_content()
time.sleep(1) # 等待1s加载
from selenium import webdriver
import time
diver=webdriver.Firefox()
diver.implicitly_wait(20)
diver.get("http://www.santostang.com/2018/07/04/hello-world")
time.sleep(5)
for i in range(1,10):
diver.execute_script("window.scrollTo(0,document.body.scrollHeight);")
diver.switch_to.frame(diver.find_element_by_css_selector("iframe[title='livere']"))
key="button[data-page="+"'"+str(i)+"'"+"]"
load_more=diver.find_element_by_css_selector(key)
load_more.click()
diver.switch_to.default_content()
time.sleep(2)
diver.switch_to.frame(diver.find_element_by_css_selector("iframe[title='livere']"))
comments=diver.find_elements_by_css_selector('div.reply-content')
for value in comments:
content=value.find_element_by_tag_name("p")
print(content.text)
diver.switch_to.default_content()

凑个热闹
194号男嘉宾
性感黄静澈，在线撩你 +Q81207848
我是三号男嘉宾付国峰
fpx牛逼
谁能打出饕餮.我就认做他爸爸

第 12 页评论:
我是38号男嘉宾
U呃ズ黄~N
大家好，我是社区管理员，请大家文明发帖，恶意灌水者，将会永久封禁您的账号，请大家维护社区环境，共建互联网美好明天！
@叶惠美 no zuo no die why you try
no try no high give me five
有没有小哥哥呢！！！！！！
@安之若素 没有呢
+vx看潘虎洗澡/
@NO NICKNAME 哇，真的吗
@安之若素 真的，还带音频
~孬夯昆咂
三元一部，十元三部，要的私我
大家好，我也是23号涛涛，加Q有福利
借你女儿还你孙子-23号
大家好我是3号邹丽

第 13 页评论:
大家好，我是23号禹涛
@Immature645 谈恋爱吗，我萝莉音哟
@水满则溢 What are you NM talking about?
浪浪
涂涂
大家好，我是禹小涛
comment
测试
给爷爪巴 测试
有人能告诉我“查看更多”在哪吗？
qqq
学习这本书不能死学，要会活用，网站上的内容在变化，url的参数有的无所谓，尤其是最后一个"_=" 多少都无所谓的，好像是跟点击次数有关。“callback=jQuery”后面这个有多个值，哪个都可以用的。总之，一定要多试几次才行，一晚上的功夫，带ajax的评论页我是没问题了，4.2节我已经学会，进我小店买东西，什么问题我都告诉你。 https://mobile.yangkeduo.com/mall_page.html?mall_id=799065059&ts=1572354791069&refer_share_id=8b9da4a9433b4353865ef8ab6b0c2db2&refer_share_uid=6447865063685&refer_share_channel=message

第 14 页评论:
挺多人在回复啊
112358
1
我也试试看
为什么抓不到
此地一游！！！此地一游！！！此地一游！！！此地一游！！！此地一游！！！此地一游！！！此地一游！！！此地一游！！！此地一游！！！此地一游！！！此地一游！！！此地一游！！！此地一游！！！此地一游！！！此地一游！！！此地一游！！！此地一游！！！此地一游！！！此地一游！！！此地一游！！！此地一游！！！此地一游！！！此地一游！！！此地一游！！！此地一游！！！此地一游！！！此地一游！！！此地一游！！！此地一游！！！此地一游！！！此地一游！！！此地一游！！！此地一游！！！此地一游！！！此地一游！！！此地一游！！！此地一游！！！此地一游！！！此地一游！！！此地一游！！！此地一游！！！此地一游！！！此地一游！！！此地一游！！！此地一游！！！此地一游！！！此地一游！！！
try_spider01
测试测试
~~~
测试抓取
2019/10/07 路人甲test

第 15 页评论:
敬敬
????
我爱你，服尔
回复一下
测试
学到头秃
我爱王椰
嘿嘿嘿
测试抓取
hhh大傻子

第 16 页评论:
测试
杨筱筠是狗屎！
有点难哦
试试解析真实地址抓取
测试网络爬虫
test
老师你好，我看的是您的第一版书，其中第64页上面您说(.*?)只匹配了smarter，但是63页印的结果是smarter than
而且我自己测试代码是也是smarter than我把(.*?)改成(.*)后也是smarter than ,去官网查看了一下，好像是*？叫懒惰匹配
因为我测试了代码的确是书上结果，但是和后面您说的不一样，是一些其他的原因吗？
@O0o0O0o0O 我又测试了下觉得可能是正则表达式中的dogs限制了原因，懒惰匹配是尽可能少的，也就意味着可能找不到返回none也是正常的，因为dogs的存在，所以限制了必须要把前面的smarter than全部匹配，如果没有dogs的话，我测试是可以的，就只是匹配了smarter
123
677
wyf的test

第 17 页评论:
4.3节用Selenium打开了网页，但是查看源代码的时候，评论那一块仍然是动态数据，有没有遇到一样问题的小伙伴交流一下啊 QQ424524128
2019.7.31
2019.07.29
2019年7月25日，Hello
2019.07.24test
2019.7.16 Test
2019.07.15 Test
2019.07.15 test
2019.7.14.test
testing

第 18 页评论:
test
test demo。。。。。。。。。。。。。。。。
简直有毒，这段代码有时候可以执行，有时候又失效了
from selenium import webdriver
driver = webdriver.Firefox(executable_path = r'C:/Users/Administrator/Desktop/geckodriver.exe')
driver.get('http://www.santostang.com/2018/07/04/hello-world/')
driver.switch_to.frame(driver.find_element_by_css_selector("iframe[title='livere']"))
comment = driver.find_element_by_css_selector('div.reply-content')
content = comment.find_element_by_tag_name('p')
print (content.text)
@风宇者 感觉是因为网速不够快没加载出来程序就提取了，应该让他睡几秒。
捣鼓了半天，终于把4.3弄出来了。有一些经验分享一下：（主要是第一个的问题，搞了很久，安装卸载程序好几次）
一个是Firefox的版本和Selenium的版本的问题。我最后安装的是Firefox的v55，然后Selenium装的是v3.5.0，因此geckodriver的版本也不是最新版（这个可以去它的官网上看，会说明适用什么版本的），这样就不会出现老师所说的还要补充自己电脑中geckodriver.exe程序的地址的步骤（我觉得这样会方便一点）。其实各个版本的对应关系还有很多，大家可以百度哈（我都是各种百度后解决的）
二是4.3中for循环中，有一行忘记缩进了（这个很明显哈，大家看到就知道了）
谢谢老师，开始学习
你好，请问为什么我使用书上第2章的爬虫代码时报错？能指导一下吗？我实在不明白soup.find().a.text.strip()中的a是什么意思。
@　Burning h1标签不是hi标签。你查hi下面他就提示你找不到a标签
你好，在做第三章爬取豆瓣top250电影时，提示：div_list = soup.find_all('div', class = 'hd') 这行代码语法错误。是因为class的原因吗？ 麻烦有时间指导一下，谢谢。
@杨崇武 应该是python版本不同，改为('div',{'class':'hd’})试一下，若不行，再同时把find_all改为findAll




我遇到问题了，出现Webdrrive error,不知道怎么解决
@NO NICKNAME 不同浏览器所需的驱动器也不一样，https://www.cnblogs.com/nancyzhu/p/8589764.html 这是我百度到的博客，你可以参考一下，我的是谷歌浏览器，作者的是火狐浏览器，不同驱动器要下载的驱动文件是不一样的



正在学到第四章动态网页爬取。json解析部分。有没有一起学习讨论下的，感觉这部分开始有点难啦。有的加个好友，QQ：656742781
来啦

第 19 页评论:
test
hello
我要做第一条评论
hi
dlutzmz
1
11
hello world
111111
@上帝、、也流泪！ 1111
hello

第 20 页评论:
测试
hello thank you
爬取所有的评论代码


import requests
import json

url = "https://api-zero.livere.com/v1/comments/list?callback=jQuery112406379165795651072_1552402149829&limit=10&offset={}&repSeq=4272904&requestPath=%2Fv1%2Fcomments%2Flist&consumerSeq=1020&livereSeq=28583&smartloginSeq=5154&_=1552402149837 "
comment_list = []
page = 1
while True:
rawtext = requests.get(url=url.format(page)).text
dicts = json.loads(rawtext[rawtext.find("{"):-2])
comments_dict = dicts.get("results").get("parents")
if len(comments_dict):
for comments in comments_dict:
comment_list.append(comments.get("content"))
page += 1
else:
break
print("一共抓取到评论%d条" % len(comment_list))
print(comment_list)
测试~
小毅来此测试：这是一条测试
阿罗哈
hi
test 2019.02.26
hhhh
这个 网站为何时而能访问，时而不能访问。是网络长城的原因吗？？

第 21 页评论:
好困呀！！一闭上眼睛就能睡着
啊

test 2019-02-08
test 2019.2.2
2019.1.29 测试评论，谢谢博主
import json
import requests
headers = {'User-Agent':'Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US; rv:1.9.1.6) Gecko/20091201 Firefox/3.5.6'}
for i in range(1,10):
url_0 = "https://api-zero.livere.com/v1/comments/list?callback=jQuery112408735667349062561_1548686253792&limit=10&offset={}&repSeq=4272904&requestPath=%2Fv1%2Fcomments%2Flist&consumerSeq=1020&livereSeq=28583&smartloginSeq=5154&_=1548686253797".format(i)
r = requests.get(url_0,headers = headers)
json_string = r.text
json_string = json_string[json_string.find('{'):-2]
json_data = json.loads(json_string)
comment_list = json_data['results']['parents']
for eachone in comment_list:
message = eachone['content']
print (message)
好累啊
能加好友一起学习吗，qq二路久咦咦咦一
2019.1.25 微腾大大的一条测试 nice

第 22 页评论:
2019.1.25
123456
19.01.17
2019-1-10 来子李雷的留言测试
还是有问题啊 网页打不开啊
test12345
6666
ooo
我来评论一下
这是一条测试评论

第 23 页评论:
这是一条测试评论
csass
这是一条测试评论
熊猫的评论 是第58条
评论试试啊
121212
1212
chrom反应慢，看得见这些评论，我就是用的chrom,还有4.3.1，没有按照唐大师说的，只要下载好解压到scripts中，就可以了，按照网上其他帖子可以打开firefox浏览器
wordpress怎样用啊
4.1 动态网页抓取 (解析真实地址 + selenium) 链接失效了

第 24 页评论:
刘一凡
后边的学习不了了吗
为什么用Chrome就看不见这些评论啊
我发现只要点4.1 4.2 就炸
崩了？
为啥就是不能加载更多啊！！！都两天了！！！
666
余大大
坚持
255

第 25 页评论:
nice to meet you
lla
点赞！
test
测试终止
测试测试1212！
测试
测试凭证
11
测试

第 26 页评论:
有这本书的学习交流群吗？
第四章问题好多，代码各种错误，求更新
测试
学习一下
读者吴先生到此一游。
第21条测试评论
第20条测试评论
第19条测试评论
第18条测试评论
第17条测试评论

第 27 页评论:
第16条测试评论
第15条测试评论
第14条测试评论
第13条测试评论
第12条测试评论
第11条测试评论
第10条测试评论
第9条测试评论
第8条测试评论
第7条测试评论

第 28 页评论:
第6条测试评论
第5条测试评论
第四条测试评论
第三条测试评论
第二条测试评论
第四章代码很多敲不出来啊 求更新
但是我把链接改成这个
http://www.santostang.com/2018/07/04/hello-world/
也只是打开浏览器,没有进入到页面,地址栏也没有地址啊
@Mr.Ming 老师，第四章只打开了浏览器，没有打开网站，怎么解决？
后面的小节也打印不出来
书中Hello world中的链接已经失效，新的Hello world 中没有评论，怎么测试呢？
4.3.1Selenium的安装与基本介绍
按照您书上的方法配置好环境,用过Anaconda和IDLE运行,都是能打开FireFox浏览器,但没有页面是FireFox的"新标签页",一会儿后浏览器自动关闭,并报如下错:
--------------------
runfile('F:/python3/Anaconda_3.py', wdir='F:/python3')
Traceback (most recent call last):

File "", line 1, in
runfile('F:/python3/Anaconda_3.py', wdir='F:/python3')

File "D:\Program Files\Anaconda3\lib\site-packages\spyder\utils\site\sitecustomize.py", line 705, in runfile
execfile(filename, namespace)

File "D:\Program Files\Anaconda3\lib\site-packages\spyder\utils\site\sitecustomize.py", line 102, in execfile
exec(compile(f.read(), filename, 'exec'), namespace)

File "F:/python3/Anaconda_3.py", line 15, in
driver = webdriver.Firefox(firefox_binary = binary, capabilities = caps)

File "D:\Program Files\Anaconda3\lib\site-packages\selenium\webdriver\firefox\webdriver.py", line 187, in __init__
self.binary, timeout)

File "D:\Program Files\Anaconda3\lib\site-packages\selenium\webdriver\firefox\extension_connection.py", line 52, in __init__
self.binary.launch_browser(self.profile, timeout=timeout)

File "D:\Program Files\Anaconda3\lib\site-packages\selenium\webdriver\firefox\firefox_binary.py", line 73, in launch_browser
self._wait_until_connectable(timeout=timeout)

File "D:\Program Files\Anaconda3\lib\site-packages\selenium\webdriver\firefox\firefox_binary.py", line 114, in _wait_until_connectable
% (self.profile.path))

WebDriverException: Can't load the profile. Possible firefox version mismatch. You must use GeckoDriver instead for Firefox 48+. Profile Dir: C:\Users\杨俊明\AppData\Local\Temp\tmpd2ot_af7 If you specified a log_file in the FirefoxBinary constructor, check it for details.
--------------------
希望您能尽快解决一下
还有就是有能出Chrome浏览器的教程(仅仅是这一节)吗
第一个评论
